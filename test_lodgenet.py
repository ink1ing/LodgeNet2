#!/usr/bin/env python
# æ ¸å¿ƒæ–‡ä»¶ï¼šæµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯LodgeNetæ¨¡å‹çš„ç»“æ„å’Œæ€§èƒ½
# LodgeNetæµ‹è¯•è„šæœ¬ï¼šç”¨äºæµ‹è¯•æ¨¡å‹æ¶æ„ã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œè¿›è¡Œæ¨ç†
# æ”¯æŒæ¨¡å‹éªŒè¯ã€æ€§èƒ½è¯„ä¼°å’Œå¯è§†åŒ–ç»“æœ

import os
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import argparse
import json
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from lodgenet_model import get_lodgenet_model, count_parameters
from dataset import CornRustDataset, get_dataloaders
from lodgenet_train import evaluate, create_dummy_segmentation_labels, CombinedSegmentationLoss
from utils import FocalLoss

def test_model_architecture():
    """
    æµ‹è¯•LodgeNetæ¨¡å‹æ¶æ„
    """
    print("====== æµ‹è¯•LodgeNetæ¨¡å‹æ¶æ„ ======")
    
    # åˆ›å»ºæ¨¡å‹
    model = get_lodgenet_model(n_channels=3, n_classes=2, img_size=128)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {count_parameters(model):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\næµ‹è¯•å‰å‘ä¼ æ’­...")
    batch_size = 2
    test_input = torch.randn(batch_size, 3, 128, 128)
    
    model.eval()
    with torch.no_grad():
        seg_output, pos_output, grade_output = model(test_input)
    
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"åˆ†å‰²è¾“å‡ºå½¢çŠ¶: {seg_output.shape}")
    print(f"ä½ç½®åˆ†ç±»è¾“å‡ºå½¢çŠ¶: {pos_output.shape}")
    print(f"ç—…å®³ç­‰çº§å›å½’è¾“å‡ºå½¢çŠ¶: {grade_output.shape}")
    
    # éªŒè¯è¾“å‡ºç»´åº¦
    assert seg_output.shape == (batch_size, 2, 128, 128), f"åˆ†å‰²è¾“å‡ºå½¢çŠ¶é”™è¯¯: {seg_output.shape}"
    assert pos_output.shape == (batch_size, 3), f"ä½ç½®åˆ†ç±»è¾“å‡ºå½¢çŠ¶é”™è¯¯: {pos_output.shape}"
    assert grade_output.shape == (batch_size, 1), f"ç—…å®³ç­‰çº§è¾“å‡ºå½¢çŠ¶é”™è¯¯: {grade_output.shape}"
    
    print("âœ… æ¨¡å‹æ¶æ„æµ‹è¯•é€šè¿‡!")
    return model

def test_loss_functions():
    """
    æµ‹è¯•æŸå¤±å‡½æ•°
    """
    print("\n====== æµ‹è¯•æŸå¤±å‡½æ•° ======")
    
    batch_size = 4
    img_size = 128
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    seg_output = torch.randn(batch_size, 2, img_size, img_size)
    seg_labels = torch.randint(0, 2, (batch_size, img_size, img_size))
    
    pos_output = torch.randn(batch_size, 3)
    pos_labels = torch.randint(0, 3, (batch_size,))
    
    grade_output = torch.randn(batch_size, 1)
    grade_labels = torch.rand(batch_size, 1) * 9  # 0-9èŒƒå›´
    
    # æµ‹è¯•æŸå¤±å‡½æ•°
    seg_criterion = CombinedSegmentationLoss()
    pos_criterion = FocalLoss(alpha=None, gamma=2.0)
    grade_criterion = nn.MSELoss()
    
    seg_loss = seg_criterion(seg_output, seg_labels)
    pos_loss = pos_criterion(pos_output, pos_labels)
    grade_loss = grade_criterion(grade_output, grade_labels)
    
    print(f"åˆ†å‰²æŸå¤±: {seg_loss.item():.4f}")
    print(f"ä½ç½®åˆ†ç±»æŸå¤±: {pos_loss.item():.4f}")
    print(f"ç—…å®³ç­‰çº§å›å½’æŸå¤±: {grade_loss.item():.4f}")
    
    print("âœ… æŸå¤±å‡½æ•°æµ‹è¯•é€šè¿‡!")

def load_and_test_model(model_path, data_root, json_root, device='cpu'):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œæµ‹è¯•
    
    å‚æ•°:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        data_root: æ•°æ®æ ¹ç›®å½•
        json_root: JSONæ ‡æ³¨æ ¹ç›®å½•
        device: è®¡ç®—è®¾å¤‡
    """
    print(f"\n====== åŠ è½½å¹¶æµ‹è¯•æ¨¡å‹: {model_path} ======")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(model_path, map_location=device)
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = get_lodgenet_model(n_channels=3, n_classes=2, img_size=128).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'N/A')}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    try:
        _, val_loader = get_dataloaders(
            data_root=data_root,
            json_root=json_root,
            batch_size=8,
            num_workers=0,
            img_size=128,
            train_ratio=0.8,
            use_extended_dataset=True
        )
        
        print(f"æµ‹è¯•é›†å¤§å°: {len(val_loader.dataset)}")
        
        # å®šä¹‰æŸå¤±å‡½æ•°
        seg_criterion = CombinedSegmentationLoss()
        pos_criterion = FocalLoss(alpha=None, gamma=2.0)
        grade_criterion = nn.MSELoss()
        
        # è¯„ä¼°æ¨¡å‹
        print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        metrics = evaluate(
            model, val_loader, seg_criterion, pos_criterion, 
            grade_criterion, device, task_weights=[0.4, 0.3, 0.3]
        )
        
        # æ‰“å°ç»“æœ
        print(f"\næ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        print(f"  æ€»æŸå¤±: {metrics['loss']:.4f}")
        print(f"  åˆ†å‰²æŸå¤±: {metrics['seg_loss']:.4f}")
        print(f"  ä½ç½®åˆ†ç±»æŸå¤±: {metrics['position_loss']:.4f}")
        print(f"  ç—…å®³ç­‰çº§å›å½’æŸå¤±: {metrics['grade_loss']:.4f}")
        print(f"  ä½ç½®åˆ†ç±»å‡†ç¡®ç‡: {metrics['position_accuracy']:.4f}")
        print(f"  ä½ç½®åˆ†ç±»F1åˆ†æ•°: {metrics['position_f1']:.4f}")
        print(f"  ä½ç½®åˆ†ç±»ç²¾ç¡®ç‡: {metrics['position_precision']:.4f}")
        print(f"  ä½ç½®åˆ†ç±»å¬å›ç‡: {metrics['position_recall']:.4f}")
        print(f"  ç—…å®³ç­‰çº§MAE: {metrics['grade_mae']:.4f}")
        print(f"  ç—…å®³ç­‰çº§RMSE: {metrics['grade_rmse']:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æŒ‡æ ‡
        target_met = (metrics['position_accuracy'] > 0.90 and 
                     metrics['position_f1'] > 0.85 and
                     metrics['position_recall'] > 0.85 and
                     metrics['position_precision'] > 0.85 and
                     metrics['grade_mae'] < 0.15 and
                     metrics['loss'] < 0.2)
        
        if target_met:
            print("\nğŸ‰ æ¨¡å‹è¾¾åˆ°ç›®æ ‡æŒ‡æ ‡!")
        else:
            print("\nâš ï¸ æ¨¡å‹æœªè¾¾åˆ°ç›®æ ‡æŒ‡æ ‡")
            print("ç›®æ ‡æŒ‡æ ‡:")
            print("  - å‡†ç¡®ç‡ > 90%")
            print("  - F1åˆ†æ•° > 0.85")
            print("  - å¬å›ç‡ > 0.85")
            print("  - ç²¾ç¡®ç‡ > 0.85")
            print("  - MAE < 0.15")
            print("  - æ€»æŸå¤± < 0.2")
        
        return metrics
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None

def visualize_predictions(model, data_loader, device, num_samples=4, save_dir=None):
    """
    å¯è§†åŒ–æ¨¡å‹é¢„æµ‹ç»“æœ
    
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°é‡
        save_dir: ä¿å­˜ç›®å½•
    """
    print(f"\n====== å¯è§†åŒ–é¢„æµ‹ç»“æœ ======")
    
    model.eval()
    samples_shown = 0
    
    # ä½ç½®ç±»åˆ«åç§°
    position_names = ['ä¸‹éƒ¨', 'ä¸­éƒ¨', 'ä¸Šéƒ¨']
    
    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))
    if num_samples == 1:
        axes = axes.reshape(1, -1)
    
    with torch.no_grad():
        for batch_idx, (images, position_labels, grade_labels) in enumerate(data_loader):
            if samples_shown >= num_samples:
                break
                
            images = images.to(device)
            position_labels = position_labels.view(-1).long().to(device)
            grade_labels = grade_labels.float().to(device)
            
            # æ¨¡å‹é¢„æµ‹
            seg_output, pos_output, grade_output = model(images)
            
            # è·å–é¢„æµ‹ç»“æœ
            seg_pred = torch.softmax(seg_output, dim=1)
            pos_pred = torch.softmax(pos_output, dim=1)
            _, pos_pred_class = torch.max(pos_output, 1)
            
            # å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬
            for i in range(min(images.size(0), num_samples - samples_shown)):
                row = samples_shown
                
                # åŸå§‹å›¾åƒ
                img = images[i].cpu().permute(1, 2, 0).numpy()
                img = (img - img.min()) / (img.max() - img.min())  # å½’ä¸€åŒ–åˆ°0-1
                axes[row, 0].imshow(img)
                axes[row, 0].set_title('åŸå§‹å›¾åƒ')
                axes[row, 0].axis('off')
                
                # åˆ†å‰²é¢„æµ‹ï¼ˆæ„ŸæŸ“åŒºåŸŸï¼‰
                seg_mask = seg_pred[i, 1].cpu().numpy()  # æ„ŸæŸ“åŒºåŸŸæ¦‚ç‡
                axes[row, 1].imshow(seg_mask, cmap='hot')
                axes[row, 1].set_title('æ„ŸæŸ“åŒºåŸŸé¢„æµ‹')
                axes[row, 1].axis('off')
                
                # ä½ç½®åˆ†ç±»ç»“æœ
                true_pos = position_labels[i].cpu().item()
                pred_pos = pos_pred_class[i].cpu().item()
                pos_conf = pos_pred[i, pred_pos].cpu().item()
                
                axes[row, 2].bar(range(3), pos_pred[i].cpu().numpy())
                axes[row, 2].set_title(f'ä½ç½®åˆ†ç±»\nçœŸå®: {position_names[true_pos]}\né¢„æµ‹: {position_names[pred_pos]} ({pos_conf:.2f})')
                axes[row, 2].set_xticks(range(3))
                axes[row, 2].set_xticklabels(position_names, rotation=45)
                
                # ç—…å®³ç­‰çº§å›å½’ç»“æœ
                true_grade = grade_labels[i].cpu().item()
                pred_grade = grade_output[i, 0].cpu().item()
                mae = abs(true_grade - pred_grade)
                
                axes[row, 3].bar(['çœŸå®', 'é¢„æµ‹'], [true_grade, pred_grade], color=['blue', 'orange'])
                axes[row, 3].set_title(f'ç—…å®³ç­‰çº§\nçœŸå®: {true_grade:.1f}\né¢„æµ‹: {pred_grade:.1f}\nMAE: {mae:.2f}')
                axes[row, 3].set_ylim(0, 9)
                
                samples_shown += 1
                if samples_shown >= num_samples:
                    break
    
    plt.tight_layout()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, 'lodgenet_predictions.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='LodgeNetæµ‹è¯•è„šæœ¬')
    
    parser.add_argument('--test_architecture', action='store_true',
                        help='æµ‹è¯•æ¨¡å‹æ¶æ„')
    parser.add_argument('--test_loss', action='store_true',
                        help='æµ‹è¯•æŸå¤±å‡½æ•°')
    parser.add_argument('--model_path', type=str, default=None,
                        help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--data_root', type=str, default='./guanceng-bit',
                        help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--json_root', type=str, default='./biaozhu_json',
                        help='JSONæ ‡æ³¨æ ¹ç›®å½•')
    parser.add_argument('--visualize', action='store_true',
                        help='å¯è§†åŒ–é¢„æµ‹ç»“æœ')
    parser.add_argument('--save_dir', type=str, default='./test_results',
                        help='ç»“æœä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•æ¨¡å‹æ¶æ„
    if args.test_architecture:
        model = test_model_architecture()
    
    # æµ‹è¯•æŸå¤±å‡½æ•°
    if args.test_loss:
        test_loss_functions()
    
    # æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
    if args.model_path:
        metrics = load_and_test_model(args.model_path, args.data_root, args.json_root, device)
        
        # å¯è§†åŒ–é¢„æµ‹ç»“æœ
        if args.visualize and metrics:
            try:
                # åŠ è½½æ¨¡å‹
                checkpoint = torch.load(args.model_path, map_location=device)
                model = get_lodgenet_model(n_channels=3, n_classes=2, img_size=128).to(device)
                model.load_state_dict(checkpoint['model_state_dict'])
                
                # åŠ è½½æ•°æ®
                _, val_loader = get_dataloaders(
                    data_root=args.data_root,
                    json_root=args.json_root,
                    batch_size=4,
                    num_workers=0,
                    img_size=128,
                    train_ratio=0.8,
                    use_extended_dataset=True
                )
                
                # å¯è§†åŒ–
                visualize_predictions(model, val_loader, device, num_samples=4, save_dir=args.save_dir)
                
            except Exception as e:
                print(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æµ‹è¯•ï¼Œé»˜è®¤æµ‹è¯•æ¶æ„
    if not any([args.test_architecture, args.test_loss, args.model_path]):
        print("æœªæŒ‡å®šæµ‹è¯•å†…å®¹ï¼Œé»˜è®¤æµ‹è¯•æ¨¡å‹æ¶æ„...")
        test_model_architecture()
        test_loss_functions()
    
    print("\n====== æµ‹è¯•å®Œæˆ ======")

if __name__ == "__main__":
    main() 